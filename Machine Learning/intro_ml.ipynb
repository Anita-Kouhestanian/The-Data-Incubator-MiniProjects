{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "from static_grader import grader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "# Part 1: Regression\n",
    "\n",
    "We'll start by looking at some [demographic and socioeconomic data](https://catalog.data.gov/dataset/rural-veterans-by-state-2015) from the Department of Veterans Affairs.  Our goal is to build a linear model that predicts an individual's household income.  Let's load in the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "!aws s3 sync s3://dataincubator-course/mldata/ . --exclude '*' --include 'vet_data.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "Scikit-learn is pretty flexible when it comes to data formats, so there is no right way to import the data.  Using [Pandas](https://pandas.pydata.org/) DataFrames makes it easier to keep track of feature names, although you may use NumPy arrays or even Python lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "vet_data = pd.read_csv(\"./vet_data.csv\", index_col=0)\n",
    "\n",
    "print(type(vet_data))\n",
    "vet_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "You can get a nice preview of the data in a DataFrame with its `.head()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "vet_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "Column descriptions can be found in this slightly unwieldy [data dictionary](https://www.va.gov/data/VAOpenDataMigration2019/Data-Dictionary-Apps-for-Ag.pdf).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "## Age Model\n",
    "\n",
    "We'll start with the first two columns in the data set.  `HINCP` is household income in dollars, and `AGEP` is each individual's age in years.  Use scikit-learn's [`LinearRegression`](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) to make a simple model that predicts income based on age.  \n",
    "\n",
    "In this case our feature matrix `X` and our label vector `y` are just the `HINCP` and `AGEP` columns from the data set.  Note that scikit-learn expects `X` to be a 2-dimensional `ndarray` or a DataFrame and `y` to be a 1-dimensional `ndarray` or a DataSeries, so we'll select these columns in slightly different ways. We'll also convert `X` and `y` to NumPy arrays to match the format of the test set used by the grader.  This isn't really necessary, but it may save you some grief if you want to write a custom estimator later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "X = vet_data[['AGEP']] # List of columns gives a DataFrame\n",
    "y = vet_data['HINCP']  # Single column gives a DataSeries\n",
    "\n",
    "X, y = X.values, y.values # Convert to NumPy arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "Now create and fit a `LinearRegression` object, and pass its `predict` method to the grader for scoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "age_est =  LinearRegression()\n",
    "age_est.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "grader.score('intro_ml__age_model', age_est.predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "Regressors in scikit-learn have a `.score` method which returns an $R^2$ score.  Let's see how well we did."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "age_est.score(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "Here the low $R^2$ score indicates underfitting.  In other words, our model isn't very good.  This isn't surprising, since we've ignored most of our data.  Furthermore, age and income may not be linearly correlated (try plotting them)! We'll get better results by including more features and using more sophisticated models.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "## Full Linear Model\n",
    "\n",
    "Next, we'll build a linear model that uses all of the features in the data set instead of just one.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "X = vet_data.drop('HINCP', axis=1).values\n",
    "y = vet_data['HINCP'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "This might seem like a repeat of the previous question, but there's a problem: all of the other features in our data set are categorical!  \n",
    "\n",
    "For example, the `marital` column represents marital status using the following codes:\n",
    "\n",
    "> 1: married\n",
    "\n",
    "> 2: divorced\n",
    "\n",
    "> 3: widowed, separated\n",
    "\n",
    "> 4: never married\n",
    "\n",
    "The numerical order of these labels isn't meaningful (someone who's divorced isn't more married than someone who's separated), so it doesn't make sense to feed them directly into a linear model.  A better alternative is to use one hot encoding, effectively creating a new indicator variable for each label.\n",
    "\n",
    "We can do this using scikit-learn's [`OneHotEncoder`](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "As an example, suppose we have a matrix with two categorical features.  The first feature takes on the values {1,2,3,4} while the second takes on the values {10,20}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "cat_feats = np.array([[1,10],[2,20],[3,10],[4,20],[3,10],[2,20],[1,10]])\n",
    "cat_feats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "Now let's run this through a one-hot encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "OneHotEncoder(categories='auto', sparse=False).fit_transform(cat_feats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "Note that the first four columns encode four values taken by the first feature, while the final two columns of the output represent the two values taken by the second feature.\n",
    "\n",
    "**Question:** Why is it necessary to fit the `OneHotEncoder`?\n",
    "\n",
    "Create a transformer to do one hot encoding on our feature matrix `X`.  \n",
    "\n",
    "We don't want to encode the `AGEP` column since that feature is already continuous.  The preferred way to do this in scikit-learn (since version 0.20) is to use the [`ColumnTransformer`](https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html).  This is a method that lets us define several transformers that might work on different columns of our feature matrix, and paste those results together into a new feature matrix.  Columns unused in any of the transformers can either be dropped (the default), or \"passed through\", i.e. pasted (unchanged) into the resulting feature matrix.  (For those familiar with it, this is similar to the [`FeatureUnion`](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.FeatureUnion.html) in scikit-learn.)  There is only a single transformer we are applying in this case, but we will use the `ColumnTransformer` in any event with the \"pass through\" option so that the `AGEP` column isn't transformed, but is included in our output feature matrix.  \n",
    "\n",
    "You should also be aware of the `sparse` argument of [`OneHotEncoder`](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html), which controls whether the output is a sparse matrix or a NumPy array.  Sparse output is more memory efficient, and it's the default, but other functions may not accept it as input.\n",
    "\n",
    "A `ColumnTransformer` takes a list of tuples of the form `(name, transformer, column(s))` where the columns for the particular transformer are specified as a list.  Don't forget to specify a proper parameter value for `remainder` in the `ColumnTransformer` to get the `AGEP` column in the output feature matrix.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(X, columns = ['AGEP', 'vet', 'period', 'HICOV', 'SEX','pregion', 'racepct',\n",
    "                                                  'marital', 'school', 'emppct', 'poverty', 'dispct', 'haskid', 'hhtype',\n",
    "                                                  'cowr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "one_hot_transformer = ColumnTransformer( transformers=[\n",
    "                        ('numeric', StandardScaler(), ['AGEP']),\n",
    "                        ('categorical', OneHotEncoder(), ['vet', 'period', 'HICOV', 'SEX','pregion', 'racepct',\n",
    "                                                  'marital', 'school', 'emppct', 'poverty', 'dispct', 'haskid', 'hhtype',\n",
    "                                                  'cowr'])],\n",
    "                    remainder='drop',\n",
    "                    n_jobs=-1\n",
    "                    )  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "The `.fit` method of the `ColumnTransformer` method calls `.fit` for each of the component transformers.  Similarly, `.transform` calls the `.transform` for each component and then pastes the resulting outputs into the new feature matrix.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "Now we could apply this transformer to our data using its `.fit_transform` method, but applying transformers directly generally isn't good practice:\n",
    "\n",
    "* Storing the original data set and the transformed data set as two different objects clutters our workspace.  There's a non-trivial chance that we'll confuse one for the other. \n",
    "* We'll need to apply the transformer again each time we encounter new data (for example, to make predictions).  This is an extra step that we might forget, and it also means that we're repeating code.\n",
    "* Both of these problems are compounded when we want to apply many transformers in sequence.    \n",
    "\n",
    "Instead, we should use scikit-learn's [`Pipeline`](http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) function to organize our transformers and estimator into a single object which only needs to be applied to the data once.  \n",
    "\n",
    "Try building a pipeline that combines our `one_hot_transformer` with a `LinearRegression()` estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "linear_est = LinearRegression()\n",
    "pipe = Pipeline([\n",
    "            (\"preprocessing\", one_hot_transformer),\n",
    "            (\"lr\", linear_est)\n",
    "       ])\n",
    "pipe.fit(df,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.predict(df).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "grader.score('intro_ml__linear_model', pipe.predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "## Polynomial Features\n",
    "\n",
    "Linear models cannot detect interactions between features. One way around this limitation is to create new features that encode the interactions we're interested in.  For example, we can use the values given by the product of each pair (or tuple) of features.  This is exactly what scikit-learn's [`PolynomialFeatures`](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html) transformer does.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "If we apply `one_hot_transformer` and `polynomial_transformer` in sequence, then we'll have more features to build a linear model on top of.  Try applying these transformers to the test data given below.  \n",
    "\n",
    "* We want to make sure that the columns of the output have the same meaning as when we apply these transformers to our training set `X`, so think about the difference between `.transform` and `.fit_transform` as you apply each transformer.\n",
    "* Make sure that you treated `AGEP` as a continuous variable when you defined `one_hot_transformer`.\n",
    "* Be sure to fit on the full training data so the transformer can see all the possible values each column can take\n",
    "* Once that is done, test on the following, much smaller, test data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "test_data = [[ 31.,   2.,   3.,   1.,   2.,   2.,   1.,   1.,   2.,   3.,   0.,   2.,   2.,   1.,   1.],\n",
    "             [ 20.,   2.,   3.,   1.,   1.,   3.,   1.,   4.,   2.,   1.,   0.,   2.,   1.,   2.,   1.],\n",
    "             [ 22.,   2.,   3.,   1.,   2.,   4.,   1.,   1.,   1.,   3.,   1.,   2.,   2.,   1.,   1.],\n",
    "             [ 73.,   1.,   3.,   1.,   1.,   1.,   1.,   1.,   2.,   3.,   0.,   1.,   1.,   1.,   1.],\n",
    "             [ 68.,   2.,   3.,   1.,   2.,   1.,   1.,   3.,   2.,   3.,   0.,   1.,   1.,   1.,   1.]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = vet_data.drop('HINCP', axis=1)\n",
    "y = vet_data['HINCP']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(test_data, columns = ['AGEP', 'vet', 'period', 'HICOV', 'SEX','pregion', 'racepct',\n",
    "                                                  'marital', 'school', 'emppct', 'poverty', 'dispct', 'haskid', 'hhtype',\n",
    "                                                  'cowr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import FeatureUnion\n",
    "union= FeatureUnion([('oh',one_hot_transformer),\n",
    "                    ('polynomial_trans', PolynomialFeatures())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "union.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_test_data = union.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "transformed_test_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "Your answer should have 5 rows and 1128 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "assert(transformed_test_data.shape == (5,1128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "grader.score('intro_ml__trans_data', lambda: transformed_test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "If you look at the output, you may notice that most of the entries are zero!    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "# The percentage of nonzero elements in our output.\n",
    "100*(transformed_test_data!=0).sum()/(5.*1128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "Unfortunately, this is an inevitable consequence of using `OneHotEncoder` and `PolynomialFeatures` together.  Most of the variables coming from one hot encoding are equal to zero most of the time, and it follows that the cross terms will equal zero with an even greater frequency.  For this reason, `PolynomialFeatures` is usually used with features which are continuous to begin with.\n",
    "\n",
    "This doesn't mean that our transformed data is useless, but variables that are zero most of the time aren't contributing as much to our model, and giving them too much weight may contribute to overfitting.  In our case, a better approach is to switch to a non-linear model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "## Decision Tree Regressor\n",
    "\n",
    "A decision tree is a binary tree that sorts data into groups by comparing observations to reference values one feature at a time.  Different features can be considered at different nodes, so decision trees are capable of encoding non-linear behavior, effectively capturing interactions between features.  \n",
    "\n",
    "Decision trees can also make a sequence of splits based on a single feature, effectively approximating non-linear functions of a single variable.  This is especially relevant because of the `AGEP` variable that we discussed above.  Intuition should tell us that the dependence of income on age is not linear.  So we'll have better luck with a non-linear model.\n",
    "\n",
    "Create a new model using [`DecisionTreeRegressor`](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "X = vet_data.drop('HINCP', axis=1)\n",
    "y = vet_data['HINCP']\n",
    "\n",
    "X, y = X.values, y.values # Convert to NumPy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.05,\n",
    "                                                    random_state=0,\n",
    "                                                    shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error as mse\n",
    "\n",
    "max_depths = range(1, 20)\n",
    "training_error = []\n",
    "for max_depth in max_depths:\n",
    "    model_1 = DecisionTreeRegressor(max_depth=max_depth)\n",
    "    model_1.fit(X, y)\n",
    "    training_error.append(mse(y, model_1.predict(X)))\n",
    "    \n",
    "testing_error = []\n",
    "for max_depth in max_depths:\n",
    "    model_2 = DecisionTreeRegressor(max_depth=max_depth)\n",
    "    model_2.fit(X_train, y_train)\n",
    "    testing_error.append(mse(y_test, model_2.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model = DecisionTreeRegressor()\n",
    "\n",
    "gs = GridSearchCV(model,\n",
    "                  param_grid = {'max_depth': range(1,21),\n",
    "                                'min_samples_split': range(10, 1000, 10)},\n",
    "                  cv=5,\n",
    "                  n_jobs=2,\n",
    "                  scoring='neg_mean_squared_error')\n",
    "\n",
    "gs.fit(X_train, y_train)\n",
    "\n",
    "print(gs.best_params_)\n",
    "print(-gs.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = DecisionTreeRegressor(max_depth=13,\n",
    "                                  min_samples_split=630)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "new_model.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "Some things to consider:\n",
    "\n",
    "* By default, a decision tree will keep growing until it has pure leaves.  For regression, this can mean that we get a different leaf for each sample.  Obviously this is a recipe for overfitting, so we should limit the growth of the tree by setting hyperparameters like `max_depth` or `min_samples_leaf`.  [`GridSearchCV`](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) can help you find the optimal values.  \n",
    "* Is one hot encoding still necessary?  Why or why not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "grader.score('intro_ml__tree_model', new_model.predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "# Part 2: Classification\n",
    "\n",
    "Next we'll look at delay and cancellation data for airline flights from 2008.  We'll only consider flights from two airline carriers: Southwest Airlines and American Airlines.  Our goal is to build a classifier that predicts which carrier each flight belongs to.\n",
    "\n",
    "Let's start by loading in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "!aws s3 sync s3://dataincubator-course/mldata/ . --exclude '*' --include 'ml_flight_data.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "flight_data = pd.read_csv(\"./ml_flight_data.csv\", index_col=0)\n",
    "\n",
    "print(type(flight_data), flight_data.shape)\n",
    "\n",
    "flight_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "A brief description of the columns can be found [here](http://stat-computing.org/dataexpo/2009/the-data.html).  For convenience, we've converted times from `hhmm` format to hours and dropped some columns.  All features can be treated as continuous.\n",
    "\n",
    "To start, we'll select the appropriate columns to get our feature and label sets, `X` and `y`.  Depending on your strategy for model selection and evaluation, you may wish to further split these into test and training sets using scikit-learn's [`train_test_split`](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "X = flight_data.drop('Carrier', axis=1).values\n",
    "y = flight_data['Carrier'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "## Logistic Regression Model\n",
    "\n",
    "We'll start by building a classifier that uses Logistic Regression.  Practically speaking, this could be very similar to the models that we built before, but we have a new challenge to deal with: our data has missing values.\n",
    "\n",
    "In a different situation, we might want to drop rows or columns, but this often means throwing out good data, and we may not have the luxury of ignoring incomplete observations when the time comes to apply our model.  Here we'll assign a value to each missing field based on the other values in the same column.  We suggest using the [`SimpleImputer`](https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html) transformer from scikit-learn's  `impute` module.  \n",
    "\n",
    "It should be noted that the behavior of `SimpleImputer` is somewhat limited.  If we wanted to use a more complicated strategy to impute values (or to use different strategies for different columns), then we'd need to write a custom transformer.  Another strategy to be aware of is filling in random values.  This is a middle ground between imputation and dropping rows/columns.          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "results =[]\n",
    "\n",
    "strategies = ['mean', 'median', 'most_frequent','constant']\n",
    "model = LogisticRegression(solver='saga', max_iter=1)\n",
    "for s in strategies:\n",
    "    pipeline = Pipeline([('impute', SimpleImputer(strategy=s)),('model', model)])\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "    scores = cross_val_score(pipeline, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "    \n",
    "    results.append(scores)\n",
    "    \n",
    "for method, accuracy in zip(strategies, results):\n",
    "    print('Method: {0}, mean accuracy: = {1:.3f}, max accuracy: {2:.3f}'.format(method, np.mean(accuracy), np.max(accuracy)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(max_iter=1000)\n",
    "pipe = Pipeline([('impute', SimpleImputer(strategy='most_frequent')),('model', model)])\n",
    "pipe.fit(X,y)\n",
    "print(\"model score: %.3f\" % pipe.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "Once you have a working pipeline, try modifying it to improve performance.\n",
    "\n",
    "* Consider the [`StandardScaler`](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) transformer.  Is it appropriate to use it here?   \n",
    "* You can use [`GridSearchCV`](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) to do hyperparameter selection.  `LogisticRegression` has hyperparameter, `C`, which controls regularization, and `SimpleImputer` has an argument, `strategy`, that controls how imputed values are calculated.     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "grader.score('intro_ml__log_model', pipe.predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "## Feature Importance\n",
    "\n",
    "One way to gain insight into our machine learning models is to look at how much influence each variable has on their predictions.  In a broad sense, variables with more influence are more important because they have more predictive power.  For example, with linear or logistic regression we can measure importance by looking at the coefficients learned by the model.  If our features were normalized to begin with, then the coefficients with the greatest absolute value correspond to the most predictive features.  \n",
    "\n",
    "In this question, we'll use the `.coef_` attribute of `LogisticRegression` to get the coefficients from the estimator we built in the previous step.  Keep in mind that we need to do feature scaling to get a fair comparison of the coefficients, so you may need to modify your pipeline to include `StandardScaler`.\n",
    "\n",
    "Then write a function that returns a list of the five most important features, together with their coefficients.  Depending on how you built your estimator, you may need to do some digging to access the `LogisticRegression` component.  The `best_estimator_` attribute of [`GridSearchCV`](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) and the `named_steps` attribute of [`Pipeline`](http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) may come in handy.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "column_names = list(flight_data.drop('Carrier', axis = 1)) \n",
    "\n",
    "top_5 = [('Distance', 0.421220771945)] * 5\n",
    "\n",
    "grader.score('intro_ml__feature_importances', top_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "*Copyright &copy; 2021 Pragmatic Institute. This content is licensed solely for personal use. Redistribution or publication of this material is strictly prohibited.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "nbclean": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
