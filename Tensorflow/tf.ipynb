{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "from static_grader import grader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "# Image Classification with TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "Image classification is a common task for deep learning and neural networks.  The raw features coming in are the pixel values.  These are simple enough to deal with, but it is difficult to connect pixel values to determining whether an image is of a cat.  Older methods used a lot of clever filters, but the current best-of-breed algorithms simply throw a lot of linear algebra at the problem.\n",
    "\n",
    "In this miniproject, you build a series of models to classify a series of images into one of ten classes. For expediency, these images are pretty small ($32\\times32\\times3$).  This can make classification a bit tricky&mdash;human performance is only about 94%.  Each of your models will be scored by comparing its accuracy to the accuracy of a reference model that we developed.  A score of 1 indicates that your model performs as well as the reference model; not that your accuracy is 100%!\n",
    "\n",
    "You will be given both a training set and a validation set.  Ground truth values are provided for the training set.  You should train your models on this set, and then make predictions for each of the validation images.  These predictions will be submitted to the grader."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "## A note on scoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "It **is** possible to score above 1 on these questions. This indicates that you've beaten our reference model&mdash;we compare our model's score on a test set to your score on a test set. See how high you can go!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "## Downloading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "We will be using the `CIFAR-10` data set.  It consists of 60,000 images, each $32\\times32$ color pixels, each belonging to one of ten classes.  The following cell will download the data, in NumPy's `.npy` format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "!aws s3 sync s3://dataincubator-course/cifar10/ ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "We can load in the data like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gzip\n",
    "\n",
    "train_images = np.load(gzip.open('train_images.npy.gz', 'rb'))\n",
    "train_labels = np.load(gzip.open('train_labels.npy.gz', 'rb'))\n",
    "validation_images = np.load(gzip.open('validation_images.npy.gz', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "The images are stored as four-dimensional arrays.  The first index indicates the image number, the second and third the $x$ and $y$ positions, and the fourth index the color channel.  Each pixel color is a floating point number between 0 and 1.  This convention allows us to view the images with matplotlib:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot\n",
    "matplotlib.pyplot.rcParams[\"axes.grid\"] = False  #  Remove the grid lines from the image.\n",
    "matplotlib.pyplot.imshow(train_images[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "The classes have already been numbered 0-9 for us; those numbers are stored in the vector `train_labels`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "train_labels[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "The human-readable names associated with this classes are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "label_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
    "               'dog', 'frog', 'horse', 'ship', 'truck']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "label_names[train_labels[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "So we can see that the image above is a frog.  (Now you see it!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "# Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "## Perceptual Delta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "Since we already have a number of labeled images, a simple approach would be to measure the difference between two images, and choose the label corresponding to nearby images.  To do this, we need to develop a metric to determine the distance between two images.  We'll make the simplifying (and completely wrong) assumption that this is just the average difference between the colors of the corresponding pixels in the two images.\n",
    "\n",
    "While we could just take RMS difference of the red, green, and blue pixels, let's be slightly more sophisticated and look at human vision for a metric.  After all, we're pretty good at image classifications, so there might be some useful optimization here.\n",
    "\n",
    "It turns out that modeling human perception is [extraordinarily complicated](https://en.wikipedia.org/wiki/Color_difference#CIEDE2000).  We're going to use a [simplified model](https://www.compuphase.com/cmetric.htm):\n",
    "\n",
    "$$\\Delta C \\equiv \\sqrt{2 \\Delta R^2 + 4 \\Delta G^2 + 3 \\Delta B^2 + \\bar R\\left(\\Delta R^2 - \\Delta B^2 \\right)} $$\n",
    "where $(R_1, G_1, B_1)$ and $(R_2, G_2, B_2)$ are the RGB components of the two colors and\n",
    "$$\\begin{align}\n",
    "\\Delta R &= R_1 - R_2 \\\\\n",
    "\\Delta G &= G_1 - G_2 \\\\\n",
    "\\Delta B &= B_1 - B_2 \\\\\n",
    "\\bar R &= \\textstyle\\frac{1}{2}\\left(R_1 + R_2\\right)\n",
    "\\end{align}$$\n",
    "\n",
    "This accounts for the fact that our eyes are most sensitive to green and least sensitive to red, and that perception is not constant with hue.\n",
    "\n",
    "Build a graph that takes in a series of images, as well as a base image, and returns a result containing the $\\Delta C$ value (for each pixel) between the base image and each image in the series.  \n",
    "\n",
    "(Note that the intention here in our solution is that `images` will be a stack of images, while `base` is a single image.  Since the function is decorated with `tf.function` the computations within the function will be recorded in a computation graph and the output from this function will be a tensor.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delta_func(images, base):\n",
    "    dr = images[:, :, :, 0] - base[:, :, 0]\n",
    "    dg = images[:, :, :, 1] - base[:, :, 1]\n",
    "    db = images[:, :, :, 2] - base[:, :, 2]\n",
    "    rmean = (images[:, :, :, 0] + base[:, :, 0])/2\n",
    "\n",
    "    DeltaC = (2*(dr**2) + 4*(dg**2) + 3*(db**2) + rmean*((dr**2) - (db**2)))**.5\n",
    "    return DeltaC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deltas = delta_func(train_images, validation_images[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances = []\n",
    "for i in range(50000):\n",
    "    distances.append(np.mean(deltas[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.argsort(np.array(distances))[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "As stated, for two images, $I_1$ and $I_2$, we define the distance between $I_1$ and $I_2$ as the average $\\Delta C$ value over the whole image, that is:\n",
    "\n",
    "$$d(I_1, I_2) = \\frac{1}{N}\\sum_{p_j} \\Delta C(p_j)$$\n",
    "where the sum is over all pixels $p_j$, $\\Delta C(p_2)$ is the $\\Delta C$ value for the pixel $p_j$, and $N$ is the total number of pixels in each image ($N= 32\\times 32$ in our case).\n",
    "\n",
    "Using `delta_func` compute the distance between the first validation image and all of the training images.\n",
    "\n",
    "**Checkpoint:** The mean value of the distances is 1.159, and the standard deviation of the distances is 0.182."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "From these, find the 100 closest images from the training set to this image.  (Note that `numpy.argsort` might help here.)  Submit a list of the indices of these images to the grader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "# indices = [43234]*100\n",
    "\n",
    "grader.score('tf__perceptual_delta', indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "**Extension:** What does this suggest about the proper class for this image?\n",
    "\n",
    "> **Aside:** Essentially, we've started to implement a $k$-nearest neighbors algorithm, using this perceptual distance as our metric.  If we ran the difference between all of the validation images and each of the training images, we could make a prediction from the nearest images for each.  Give it a try, if you're interested, but this miniproject is going to go in another direction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "## Smallest delta model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "Instead of comparing each validation image to each training image, let's generate a \"typical\" image for each class.  This typical image will be the one that minimizes the total (or average) perceptual delta from all training images in this class.  We could do that through some clever averaging, but let's treat it as a minimization problem so that we can do gradient descent.  \n",
    "\n",
    "This problem will have two steps, the first step is to train a typical image for each class in the training set, the second will be to make predictions on the validation set.  These predictions will be based on the closest \"typical\" image trained in the first step.\n",
    "\n",
    "Make the typical image a TensorFlow variable, starting with random pixel values.  It's this TensorFlow variable that we will train by performing gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "sample = tf.Variable(np.load(open('typical_6.npy','rb')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "Define an error function that measures the difference between that typical image and a set of images. You should be able to use the same `delta_func` you defined in the first problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "!aws s3 cp s3://dataincubator-course/miniprojects/tf/typical_6.npy .\n",
    "matplotlib.pyplot.imshow(np.load(open('typical_6.npy','rb')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frogs = np.where(train_labels == 6)[0]   #  Get the indices for the frogs\n",
    "\n",
    "fig, ax = matplotlib.pyplot.subplots(2, 2)\n",
    "for i in range(4):\n",
    "    np.ravel(ax)[i].imshow(train_images[ frogs[i] ])\n",
    "\n",
    "matplotlib.pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "Now, we will use gradient descent to find a typical image in order to minimize the error.  We want to do this for each of the ten classes of images, but let's first concentrate on how we do this for a single class (we can roll the code into a `for` loop later).  \n",
    "\n",
    "First let's get the images that belong to a single class, say class 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "typical_ims = []\n",
    "for i in range(10):\n",
    "    img = tf.Variable(tf.random.uniform((32,32,3), dtype=tf.float64))\n",
    "    label = i\n",
    "    pics = np.where(train_labels == label)\n",
    "    ims = train_images[pics]\n",
    "\n",
    "    ##optimizer\n",
    "    optimizer = tf.keras.optimizers.SGD(learning_rate=0.07)\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = delta_func(ims, img)\n",
    "    vars_l = [img]\n",
    "    grads = tape.gradient(loss, vars_l)\n",
    "\n",
    "    # Process the gradients, for example cap them, etc.\n",
    "    # capped_grads = [MyCapper(g) for g in grads]\n",
    "    #processed_grads = [process_gradient(g) for g in grads]\n",
    "\n",
    "    # Ask the optimizer to apply the processed gradients.\n",
    "    optimizer.apply_gradients(zip(grads, vars_l))\n",
    "    typical_ims.append(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "typical_ims = [tf.cast(typical_ims[i], tf.float32) for i in range(10)]\n",
    "typical_ims[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "typical_ims[1].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = [c_delta(typical_ims[i],tf.cast(validation_images[0], tf.float32)).numpy() for i in range(10)]\n",
    "np.argmin(np.asarray(diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_classes = []\n",
    "for im in validation_images:\n",
    "    im = tf.cast(im, tf.float32)\n",
    "    diff = [c_delta(typical_ims[i],im).numpy() for i in range(10)]\n",
    "    predicted_classes.append(np.argmin(np.asarray(diff)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "Now define the gradient descent optimizer, and a function that will calculate the gradient of the error with respect to your typical image, using the optimizer to apply the gradient to the typical image.  Remember that you need to specify a learning rate for the optimizer.  You might need to later experiment with the learning rate to find the right value so the training will converge to the minimum (and converge quickly).  (__Note:__  \"Normal\" learning rates are typically in the range 0.1 to 1, perhaps even smaller in some situations.  The learning rate we used in our solution was 10.)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "len(predicted_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "Keeping in mind that our training set (for a single label) consists of 5,000 images, let's perform the training in batches.  To ensure good training, let's pick our batches randomly from the set of training images.  \n",
    "\n",
    "Strictly speaking, we are performing a batched version of __stochastic gradient descent__.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "__Checkpoint:__  Here's the typical image we generated for label 6 when we did the training.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "You can compare this to the first four frog (class label 6) training images. A perfect fit, right?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "Having now done this training for one class, it should be easy to roll this code into a `for` loop to generate a typical image for each of the 10 classes.  Don't forget to save your typical image for each class, as they are needed for the prediction step.\n",
    "\n",
    "**Hint:** We suggest storing your typical images in a file using the `numpy` \"save\" function.  That function allows you to save a `numpy` array to a binary file.  That way if you need to restart this notebook, you can reload your typical images you have already trained (not having to re-run the code above to create these typical images again).  (We used `numpy.load` near the start of this notebook to load the training and validation images that were stored in this binary format.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "Finally, we're ready to make a prediction for each validation image based on which typical image is nearest under our distance metric.  You need to generate a list of 10,000 integers, representing the predicted class of each validation image.   \n",
    "\n",
    "__Hint:__ Use `delta_func` you defined from question 1 to find the distance between each validation image and a _single_ typical image, and repeat for each typical image to create a `numpy` array of distances having shape (10000, 10).  You should then find the `np.argmin()` function helpful in finding the predicted label for each validation image.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "# predicted_classes = return [0]*len(validation_images)\n",
    "\n",
    "grader.score('tf__smallest_delta', predicted_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "## Softmax model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "Looking only at the smallest distance is throwing some information away.  We would expect some classes to have more variation that others.  Also, we would expect correlation between the classes&mdash;a small distance to *cat* is probably more likely for *dog* than for *airplane*.\n",
    "\n",
    "Instead of a trying to work out these effects heuristically, let the computer do it.  Build a softmax model that takes as input features the distances from all ten typical images and makes a class prediction from those.  Again, use this to predict the class for each of the validation images.\n",
    "\n",
    "**Hint:**\n",
    "- The labels are given as integers, but softmax expects one-hot encoding of the labels.  The `tf.one_hot` function can do the conversion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "class LogisticRegression():\n",
    "    def __init__(self, eta=.1):\n",
    "        self.opt = tf.keras.optimizers.SGD(learning_rate=eta)\n",
    "    \n",
    "    def _logits(self, X):\n",
    "        return tf.matmul(X, self.W) + self.b\n",
    "    \n",
    "    def loss(self, X, y, return_func=False):\n",
    "        def loss_():\n",
    "            return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "                logits=self._logits(X), labels=y))\n",
    "        \n",
    "        if not return_func:\n",
    "            return loss_()\n",
    "        \n",
    "        return loss_\n",
    "    \n",
    "    def fit(self, X, y, steps=100000):\n",
    "        if not hasattr(self, 'W'):\n",
    "            self.W = tf.Variable(tf.zeros((X.shape[1], 10), dtype=X.dtype))\n",
    "        if not hasattr(self, 'b'):\n",
    "            self.b = tf.Variable(tf.zeros((1,10), dtype=X.dtype))\n",
    "        \n",
    "        for _ in range(steps):\n",
    "            self.opt.minimize(self.loss(X, y, return_func=True), [self.W, self.b])\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        return tf.nn.softmax(self._logits(X))\n",
    "        \n",
    "    def predict(self, X):\n",
    "        return tf.argmax(self.predict_proba(X), axis = 1)\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        return tf.reduce_mean(tf.cast(tf.equal(self.predict(X), tf.argmax(y,axis=1)), tf.float64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = []\n",
    "for im in train_images:\n",
    "    im = tf.cast(im, tf.float32)\n",
    "    diff = np.asarray([c_delta(typical_ims[i],im).numpy() for i in range(10)])\n",
    "    X_train.append(diff) \n",
    "X_train = np.asarray(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = tf.one_hot(train_labels, 10).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = []\n",
    "for im in validation_images:\n",
    "    im = tf.cast(im, tf.float32)\n",
    "    diff = np.asarray([c_delta(typical_ims[i],im).numpy() for i in range(10)])\n",
    "    X_test.append(diff) \n",
    "X_test = np.asarray(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = tf.one_hot(train_labels[40000:], 10).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_classes = model.predict(X_test)\n",
    "predicted_classes = list(predicted_classes.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "# predicted_classes = return [0]*len(validation_images)\n",
    "\n",
    "grader.score('tf__softmax', predicted_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "**Extension:** Does this perform any better than the previous solution?  What metric is the right one to use to make this judgment?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "## Fully-connected model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "The previous model still makes the assumption that this distance metric is the right measure to use.  In this problem, we'll give this up.  Build a multi-layer fully-connected neural network that takes the pixel values as input and yields a class prediction as output.\n",
    "\n",
    "**Hints:**\n",
    "- We found that adding more layers didn't help too much.\n",
    "- Watch out for overfitting.  Dropout can help with this.\n",
    "- The reference solution achieves an accuracy of about 44% on a training set and 41% on a test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = [np.concatenate(np.concatenate(im)) for im in train_images]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train_labels[:49000]\n",
    "\n",
    "X_t = np.asarray(X_train[:49000])\n",
    "\n",
    "X_test = np.asarray(X_train[49000:])\n",
    "y_test = train_labels[49000:]\n",
    "                          \n",
    "from tensorflow.keras.utils import to_categorical as one_hot\n",
    "\n",
    "y_train_hot = one_hot(y_train)\n",
    "y_test_hot = one_hot(y_test)\n",
    "                          \n",
    "\n",
    "N_PIXELS= 32 * 32 * 3\n",
    "N_CLASSES = 10\n",
    "\n",
    "hidden_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "\n",
    "model.add(\n",
    "    keras.layers.Dense(\n",
    "        hidden_size,\n",
    "        activation='sigmoid',\n",
    "        use_bias=True,  # The default\n",
    "        kernel_initializer=keras.initializers.TruncatedNormal(stddev=N_PIXELS**-0.5)\n",
    "    )\n",
    ")\n",
    "\n",
    "model.add(\n",
    "    keras.layers.Dense(\n",
    "        10,\n",
    "        activation='softmax',\n",
    "        kernel_initializer=keras.initializers.TruncatedNormal(stddev=hidden_size**-0.5)\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=keras.optimizers.SGD(lr=0.5),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_t, y_train_hot,\n",
    "                    epochs=30,\n",
    "                    batch_size=128,\n",
    "                    validation_data=(X_test,y_test_hot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = [np.concatenate(np.concatenate(im)) for im in validation_images]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_classes = list(np.argmax(model.predict(np.asarray(val)),axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "# predicted_classes = return [0]*len(validation_images)\n",
    "\n",
    "grader.score('tf__fully_connected', predicted_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "## Convolutional model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "Convolutional neural networks have had a lot of success in image classification.  Build a neural network with convolutional layers to improve the performance.\n",
    "\n",
    "**Hints:**\n",
    "- The reference solution uses two convolutional layers and two fully-connected layers.\n",
    "- We found success with the `AdamOptimizer`.\n",
    "- The reference solution achieves an accuracy of roughly 80% on a training set and 70% on a test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "X_train = [np.concatenate(np.concatenate(im)) for im in train_images]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train_labels[:48000]\n",
    "\n",
    "X_t = np.asarray(X_train[:48000])\n",
    "\n",
    "X_test = np.asarray(X_train[48000:])\n",
    "y_test = train_labels[48000:]\n",
    "                          \n",
    "from tensorflow.keras.utils import to_categorical as one_hot\n",
    "\n",
    "y_train_hot = one_hot(y_train)\n",
    "y_test_hot = one_hot(y_test)\n",
    "                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 32\n",
    "img_size_flat = img_size * img_size\n",
    "img_shape = (img_size, img_size)\n",
    "\n",
    "n_classes = 10\n",
    "n_channels = 1\n",
    "filt_size = [5, 5] # 5x5 pixel filters\n",
    "\n",
    "batch_size = 128\n",
    "n_epochs = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "\n",
    "model.add(keras.layers.Reshape([img_size, img_size, 3]))\n",
    "model.add(keras.layers.Conv2D(16, filt_size, padding='same',\n",
    "                              activation='relu'))\n",
    "\n",
    "model.add(keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2,2),\n",
    "                                    padding='same'))\n",
    "\n",
    "model.add(keras.layers.Dropout(0.4))\n",
    "\n",
    "model.add(keras.layers.Flatten())\n",
    "\n",
    "model.add(keras.layers.Dense(200, activation='relu'))\n",
    "\n",
    "model.add(keras.layers.Dense(n_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_t, y_train_hot,\n",
    "                    epochs=n_epochs,\n",
    "                    batch_size=batch_size,\n",
    "                    validation_data=(X_test,y_test_hot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = [np.concatenate(np.concatenate(im)) for im in validation_images]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_classes = list(np.argmax(model.predict(np.asarray(val)),axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "\n",
    "# predicted_classes = return [0]*len(validation_images)\n",
    "\n",
    "grader.score('tf__convolutional', predicted_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "## Transfer learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "In transfer learning, we use a network trained on one data set to provide a starting point for the modeling of other data.  As we are trying to model color images, we should look for another network trained on color images.  Luckily, we have already discussed such a network: the Inception network used in the Deep Dream notebook.\n",
    "\n",
    "The following cell will load the model, omitting its classification layer (since we're not interested in classifying `ImageNet` images)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "# include_top=False will discard avg_pool before prediction layer\n",
    "inception = tf.keras.applications.inception_v3.InceptionV3(include_top=True, input_shape=(299, 299, 3))\n",
    "inception = tf.keras.Model([inception.input], [inception.layers[-2].output]) # manually discard prediction layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "In a transfer learning setup, we will use the first part of the trained network to process the original images, and then train a network to make predictions from the output of the trained network.  There are several ways to accomplish this.\n",
    "\n",
    "One approach is to connect the new layers to the output of the existing layers.  Data will flow through the pre-trained layers as well as those added.  In the training step, only the new layers should be marked as trainable.\n",
    "\n",
    "However, this can be a bit wasteful when multiple epochs of training will be undertaken since we recalculate the latent vectors on every training step.  With a smaller data set, such as this, it can be more efficient to pre-calculate the *latent vectors* that are the output of the pre-trained network.  These can be stored and used as input for training a smaller, separate network to make the predictions.  We recommend this approach for this miniproject.\n",
    "\n",
    "Images should be fed to the `inception` network and then vectorized (you might want to refer to the `TF_DeepDream.ipynb` notebook)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "We've loaded the `inception` network with its native image shape: $299 \\times 299$.  This implies that we need to upscale our images from $32\\times32$ to $299\\times299$.  There are a number of ways to do this; the reference solution uses `tf.image.resize` with bilinear interpolation.  (More sophisticated resizing methods produce better results, but will take significantly longer!)\n",
    "\n",
    "It may make sense to do the rescaling and latent vector calculation at the same time, to avoid storing the (somewhat large) rescaled images unnecessarily.  You also probably want to save those latent vectors to disk, to avoid the need to repeat this calculation later.\n",
    "\n",
    "**Hints:**\n",
    "- Be sure to batch this calculation; resizing all 50,000 images at once will cause memory errors.\n",
    "- The latent vector calculation took us between 30 minutes and 2 hours on a single machine.  You might consider distributing the calculation.\n",
    "- The latent vectors for the first 10 images have an average of 1983 non-zero values and an overall average value of 0.319."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "With the latent vectors calculated, we can use them as input to train a small neural network to make the final predictions.\n",
    "\n",
    "**Hints:**\n",
    "- The reference solution has three layers.\n",
    "- The reference solution achieves a training accuracy of 87% and a test accuracy of 85%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(20):\n",
    "    images = train_images[i*2500: (i+1)*2500]\n",
    "    lis = [tf.image.resize(im,[299, 299]).numpy().astype(np.float32) for im in images]\n",
    "    res = [inception.predict(np.expand_dims(l, axis=0)) for l in lis]\n",
    "    res_ready = np.asarray(res)\n",
    "    with open('resize{}.npy'.format(i), 'wb') as f:\n",
    "        np.save(f, res_ready)\n",
    "    del res_ready\n",
    "    del images\n",
    "    del lis\n",
    "    del res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load all files together\n",
    "ns = []\n",
    "for i in range(20):\n",
    "    a = np.load(open(\"resize{}.npy\".format(i),'rb'))\n",
    "    ns.extend(a)#.reshape(-1,2048))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_vects = np.concatenate(ns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = ns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train_labels[:48000]\n",
    "\n",
    "X_t = np.concatenate(X_train[:48000])\n",
    "\n",
    "X_test = np.concatenate(X_train[48000:])\n",
    "y_test = train_labels[48000:]\n",
    "                          \n",
    "from tensorflow.keras.utils import to_categorical as one_hot\n",
    "\n",
    "y_train_hot = one_hot(y_train)\n",
    "y_test_hot = one_hot(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_PIXELS= 299 * 299 * 3\n",
    "N_CLASSES = 10\n",
    "\n",
    "hidden_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "\n",
    "model.add(\n",
    "    keras.layers.Dense(\n",
    "        hidden_size,\n",
    "        activation='sigmoid',\n",
    "        use_bias=True,  # The default\n",
    "        kernel_initializer=keras.initializers.TruncatedNormal(stddev=N_PIXELS**-0.5)\n",
    "    )\n",
    ")\n",
    "\n",
    "model.add(\n",
    "    keras.layers.Dense(\n",
    "        10,\n",
    "        activation='softmax',\n",
    "        kernel_initializer=keras.initializers.TruncatedNormal(stddev=hidden_size**-0.5)\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=keras.optimizers.SGD(lr=0.5),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_t, y_train_hot,\n",
    "                    epochs=2,\n",
    "                    batch_size=128,\n",
    "                    validation_data=(X_test,y_test_hot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lis = [tf.image.resize(im,[299, 299]).numpy().astype(np.float32) for im in validation_images[:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    images = validation_images[i*1000: (i+1)*1000]\n",
    "    lis = [tf.image.resize(im,[299, 299]).numpy().astype(np.float32) for im in images]\n",
    "    res = [inception.predict(np.expand_dims(l, axis=0)) for l in lis]\n",
    "    res_ready = np.asarray(res)\n",
    "    with open('test{}.npy'.format(i), 'wb') as f:\n",
    "        np.save(f, res_ready)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = []\n",
    "for i in range(10):\n",
    "    a = np.load(open(\"test{}.npy\".format(i),'rb'))\n",
    "    test.extend(a)#.reshape(-1,2048))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = np.concatenate(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_classes = list(np.argmax(model.predict(val),axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "# predicted_classes = return [0]*len(validation_images)\n",
    "\n",
    "grader.score('tf__transfer_learning', predicted_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "*Copyright &copy; 2020 Pragmatic Institute. This content is licensed solely for personal use. Redistribution or publication of this material is strictly prohibited.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "nbclean": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
